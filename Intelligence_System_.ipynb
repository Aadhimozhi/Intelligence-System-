{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+Ocdu7lkhjuIUQjp5zYdM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aadhimozhi/Intelligence-System-/blob/main/Intelligence_System_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai numpy"
      ],
      "metadata": {
        "id": "riXxJrE5oejF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ==============================\n",
        "# CONFIG\n",
        "# ==============================\n",
        "\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"   # Local AI embedding model\n",
        "TOP_K = 3\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "# ==============================\n",
        "# LOAD NOTES\n",
        "# ==============================\n",
        "\n",
        "def load_notes(path=\"notes.txt\"):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# ==============================\n",
        "# TEXT CHUNKING\n",
        "# ==============================\n",
        "\n",
        "def chunk_text(text, size=500, overlap=100):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + size\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "# ==============================\n",
        "# EMBEDDINGS\n",
        "# ==============================\n",
        "\n",
        "def embed_texts(texts):\n",
        "    return model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "# ==============================\n",
        "# COSINE SIMILARITY\n",
        "# ==============================\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# ==============================\n",
        "# SEMANTIC SEARCH\n",
        "# ==============================\n",
        "\n",
        "def semantic_search(query, chunks, embeddings, k=3):\n",
        "    query_emb = model.encode(query, convert_to_numpy=True)\n",
        "    scores = [cosine_similarity(query_emb, emb) for emb in embeddings]\n",
        "    top_idx = np.argsort(scores)[-k:][::-1]\n",
        "    return [chunks[i] for i in top_idx]\n",
        "\n",
        "# ==============================\n",
        "# SHORT ANSWER GENERATION\n",
        "# ==============================\n",
        "\n",
        "def generate_answer(question, retrieved_chunks):\n",
        "    if not retrieved_chunks:\n",
        "        return \"Information not available in the notes.\"\n",
        "\n",
        "    # Split chunks into sentences\n",
        "    sentences = []\n",
        "    for chunk in retrieved_chunks:\n",
        "        sentences.extend(re.split(r'(?<=[.!?])\\s+', chunk))\n",
        "\n",
        "    # Extract keywords\n",
        "    keywords = question.lower().split()\n",
        "\n",
        "    # Score sentences\n",
        "    scored = []\n",
        "    for sent in sentences:\n",
        "        score = sum(1 for word in keywords if word in sent.lower())\n",
        "        if score > 0:\n",
        "            scored.append((score, sent.strip()))\n",
        "\n",
        "    if not scored:\n",
        "        return \"Information not available in the notes.\"\n",
        "\n",
        "    # Sort by relevance\n",
        "    scored.sort(reverse=True)\n",
        "\n",
        "    # Return top 2 sentences only\n",
        "    answer = \" \".join([s[1] for s in scored[:2]])\n",
        "    return answer\n",
        "\n",
        "# ==============================\n",
        "# QA PIPELINE\n",
        "# ==============================\n",
        "\n",
        "def answer_question(question, chunks, embeddings):\n",
        "    retrieved = semantic_search(question, chunks, embeddings, TOP_K)\n",
        "\n",
        "    if not retrieved:\n",
        "        return \"Information not available in the notes.\"\n",
        "\n",
        "    answer = generate_answer(question, retrieved)\n",
        "\n",
        "    if \"Information not available\" in answer:\n",
        "        return answer\n",
        "\n",
        "    return answer + \"\\n\\nSource: \" + \", \".join(\n",
        "        [f\"Chunk {i+1}\" for i in range(len(retrieved))]\n",
        "    )\n",
        "\n",
        "# ==============================\n",
        "# RUN SYSTEM\n",
        "# ==============================\n",
        "\n",
        "notes = load_notes()\n",
        "chunks = chunk_text(notes)\n",
        "\n",
        "print(\"üìê Creating AI embeddings...\")\n",
        "embeddings = embed_texts(chunks)\n",
        "\n",
        "print(\"‚úÖ System Ready (type 'exit' to stop)\\n\")\n",
        "\n",
        "while True:\n",
        "    q = input(\"‚ùì Question: \")\n",
        "    if q.lower() == \"exit\":\n",
        "        print(\"üëã System stopped.\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nüß† Answer:\")\n",
        "    print(answer_question(q, chunks, embeddings))\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwdONrTSqpUL",
        "outputId": "b5c70840-5e12-47db-cc7a-006933347244"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìê Creating AI embeddings...\n",
            "‚úÖ System Ready (type 'exit' to stop)\n",
            "\n",
            "‚ùì Question: Who invented transformers?\n",
            "\n",
            "üß† Answer:\n",
            "Information not available in the notes.\n",
            "------------------------------------------------------------\n",
            "‚ùì Question: exit\n",
            "üëã System stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6F1OHwXmuwyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}